## 信息熵

1. 记录系统的状态数多少，判断需要多少个存储的二进制位。
2. 通常，一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。概率大，出现机会多，不确定性小；反之就大。
3. 有序的系统比无序的系统熵要小



不确定性函数f是概率P的单调递降函数；两个独立符号所产生的不确定性应等于各自不确定性之和，即f（P1，P2）=f（P1）+f（P2），这称为可加性。同时满足这两个条件的函数f是对数函数，即![img](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D154/sign=2ffd8750a7c27d1ea1263fc12fd4adaf/b219ebc4b74543a979e4cf381d178a82b8011442.jpg)



在信源中，考虑的不是某一单个符号发生的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。若信源符号有n种取值：U1…Ui…Un，对应概率为：P1…Pi…Pn，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性-logPi的统计平均值（E），可称为信息熵，即![img](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D244/sign=49cc170a35d3d539c53d08c70e86e927/2e2eb9389b504fc2554487f1e6dde71190ef6d2e.jpg)

式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。

 

![img](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike72%2C5%2C5%2C72%2C24/sign=d26d4854f503738dca470470d272db34/ac345982b2b7d0a267983369c8ef76094a369af4.jpg)

离散信源的信息熵具有：

①非负性，即收到一个信源符号所获得的信息量应为正值，H（U）≥0；

②对称性，即对称于P=0．5

③确定性，H（1，0）=0，即P=0或P=1已是确定状态，所得信息量为零；

④极值性，当P=0．5时，H（U）最大；而且H（U）是P的上凸函数。